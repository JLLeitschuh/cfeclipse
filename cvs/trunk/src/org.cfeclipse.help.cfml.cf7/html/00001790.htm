<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html>
<head>
   <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
   <meta http-equiv="Content-Style-Type" content="text/css">
   <script src="pages.js" type="text/javascript" language="Javascript1.2" charset="UTF-8"></script>
   <title>Path and URL options</title>
</head>
<body>
<p align="right"><p align="right"><a href="00001788.htm"><img src="images/previous.gif" width="9" height="14" border="0" alt="Previous"></a>&nbsp;&nbsp;<a href="00001791.htm"><img src="images/next.gif" width="9" height="14" border="0" alt="Next"></a>
</p></p>
<hr />
<h1>Path and URL options</h1>
<p>The following sections describe the Verity Spider path and URL options.</p>
<h3><a name="wp1000055"></a>-auth</h3>
<p><strong>Syntax</strong>: <code>-auth path_and_filename</code></p>
<p>Specifies an authorization file to support authentication for secure paths. </p>
<p>Use the <code>-auth</code> option to specify the authorization file. The file contains one record per line. Each line consists of server, realm, username, and password, separated by whitespace.</p>
<p>The following is a sample authorization file:</p>
<pre># This is the Authorization file for HTTP&#39;s Basic Authentication
#server&#160;&#160;&#160;realm&#160;&#160;&#160;&#160;username&#160;&#160;&#160;&#160;&#160;&#160;password
doleary&#160;&#160;&#160;MACR&#160;&#160;&#160;&#160;&#160;my_username&#160;&#160;&#160;my_password
</pre><h3><a name="wp1000049"></a>-cgiok</h3>
<p><strong>Type</strong>: Web crawling only</p>
<p>Lets you index URLs containing query strings. That is, a question mark (?) followed by additional information. This typically means that the URL leads to a CGI or other processing program.</p>
<p>The return document produced by the web server is indexed and parsed for document links, which are followed and in turn indexed and parsed. However, if the web server does not return a page, perhaps because the URL is missing parameters that are required for processing in order to produce a page, nothing happens. There is no page to index and parse.</p>
<h4><a name="wp1005691"></a>Example</h4>
<p>The following is a URL without parameters:</p>
<pre>http://server.com/cgi-bin/program?
</pre><p>If you include parameters in the URL to be indexed, as specified with the <code>-start</code> option, those parameters are processed and any resulting pages are indexed and parsed.</p>
<p>By default, a URL with a question mark (?) is skipped.</p>
<h3><a name="wp1000043"></a>-domain</h3>
<p><strong>Type</strong>: Web crawling only</p>
<p><strong>Syntax</strong>: <code>-domain name_1 [name_n] ...</code></p>
<p>Limits indexing to the specified domain(s). You must use only complete text strings for domains. You cannot use wildcard expressions. URLs not in the specified domain(s) are not downloaded or parsed.</p>
<p>You can list multiple domains by separating each one with a single space.</p>
<p><strong>Note: </strong>You must have the appropriate Verity Spider licensing capability to use this option. The Verity Spider that is included with ColdFusion MX is licensed for websites that are defined and reside on the same machine on which ColdFusion MX is installed. Contact Verity Sales for licensing options regarding the use of Verity Spider for external websites.</p><h3><a name="wp1024888"></a>-followdup</h3>
<p>Specifies that Verity Spider follows links within duplicate documents, although only the first instance of any duplicate documents is indexed.</p>
<p>You might find this option useful if you use the same home page on multiple sites. By default, only the first instance of the document is indexed, while subsequent instances are skipped. If you have different secondary documents on the different sites, using the <code>-followdup</code> option lets you get to them for indexing, while still indexing the common home page only once.</p>
<h3><a name="wp1024892"></a>-followsymlink</h3>
<p><strong>Type</strong>: File system only</p>
<p>Specifies that Verity Spider follows symbolic links when indexing UNIX file systems.</p>
<h3><a name="wp1000025"></a>-host</h3>
<p><strong>Type</strong>: Web crawling only</p>
<p><strong>Syntax</strong>: <code>-host name_1 [name_n] ...</code></p>
<p>Limits indexing to the specified host or hosts. You must use only complete text strings for hosts. You cannot use wildcard expressions.</p>
<p>You can list multiple hosts by separating each one with a single space. URLs not on the specified host(s) are not downloaded or parsed.</p>
<h3><a name="wp1000019"></a>-https</h3>
<p><strong>Type</strong>: Web crawling only</p>
<p>Lets you index SSL-enabled websites.</p>
<p><strong>Note: </strong>You must have the Verity SSL Option Pack installed to use the <code>-https</code> option. The Verity SSL Option Pack is a Verity Spider add-on available separately from a Verity salesperson.</p><h3><a name="wp1000013"></a>-jumps</h3>
<p><strong>Type</strong>: Web crawling only</p>
<p><strong>Syntax</strong>: <code>-jumps num_jumps</code></p>
<p>Specifies the maximum number of levels an indexing job can go from the starting URL. Specify a number between 0 and 254.</p>
<p>The default value is unlimited. If you see extremely large numbers of documents in a collection where you do not expect them, consider experimenting with this option, in conjunction with the Content options, to pare down your collection.</p>
<h3><a name="wp1000007"></a>-nodocrobo</h3>
<p>Specifies to ignore ROBOT META tag directives.</p>
<p>In HTML 3.0 and earlier, robot directives could only be given as the file robots.txt under the root directory of a website. In HTML 4.0, every document can have robot directives embedded in the META field. Use this option to ignore them. Use this option with discretion.</p>
<h3><a name="wp1000310"></a>-nofollow</h3>
<p><strong>Type</strong>: Web crawling only</p>
<p><strong>Syntax</strong>: <code>-nofollow &quot;exp&quot;</code></p>
<p>Specifies that Verity Spider cannot follow any URLs that match the exp expression. If you do not specify an exp value for the <code>-nofollow</code> option, Verity Spider assumes a value of &quot;*&quot;, where no documents are followed.</p>
<p>You can use wildcard expressions, where the asterisk (*) is for text strings and the question mark (?) is for single characters. Always encapsulate the exp values in double-quotation marks to ensure that they are properly interpreted. </p>
<p>If you use backslashes, you must double them so that they are properly escaped; for example:</p>
<pre>C:\\test\\docs\\path
</pre><p>To use regular expressions, also specify the <a href="00001787.htm#999864">-regexp</a> option.</p>
<p>Earlier versions of Verity Spider did not allow the use of an expression. This meant that for each starting point URL, only the first document would be indexed. With the addition of the expression functionality, you can now selectively skip URLs, even within documents.</p>
<p>See also <a href="00001787.htm#999864">-regexp</a></p>
<h3><a name="wp1000304"></a>-norobo</h3>
<p>Type: Web crawling only</p>
<p>Specifies to ignore any robots.txt files encountered. The robots.txt file is used on many websites to specify what parts of the site indexers should avoid. The default is to honor any robots.txt files.</p>
<p>If you are re-indexing a site and the robots.txt file has changed, Verity Spider deletes documents that have been newly disallowed by the robots.txt file.</p>
<p>Use this option with discretion and extreme care, especially in conjunction with the <a href="00001790.htm#1000049">-cgiok</a> option.</p>
<p>See also <a href="00001790.htm#1000007">-nodocrobo</a>.</p>
<h3><a name="wp1000298"></a>-pathlen</h3>
<p><strong>Syntax</strong>: <code>-pathlen num_pathsegments</code></p>
<p>Limits indexing to the specified number of path segments in the URL or file system path. The path length is determined as follows:</p>
<ul>
<li>
   The host name and drive letter are not included; for example, neither www.spider.com:80/ nor C:\ would be included in determining the path length. 
</li>
<li>
   All elements following the host name are included. 
</li>
<li>
   The actual filename, if present, is included; for example, /world.html would be included in determining the path length. 
</li>
<li>
   Any directory paths between the host and the actual filename are included. 
</li>
</ul>
<h4><a name="wp1006349"></a>Example</h4>
<p>For the following URL, the path length would be four: </p>
<pre>http://www.spider:80/comics/fun/funny/world.html
       &lt;-1-&gt;          &lt;2&gt;  &lt;-3-&gt; &lt;---4---&gt;
</pre><p>For the following file system path, the path length would be three:</p>
<pre>C:\files\docs\datasheets
    &lt;-1-&gt;&lt;-2-&gt;&lt;---3---&gt;
</pre><p>The default value is 100 path segments.</p>
<h3><a name="wp1000292"></a>-refreshtime</h3>
<p><strong>Syntax</strong>: <code>-refreshtime timeunits</code></p>
<p>Specifies not to refresh any documents that have been indexed since the timeunits value began. </p>
<p>The following is the syntax for timeunits: </p>
<pre>n day n hour n min n sec
</pre><p>Where n is a positive integer. You must include spaces, and since the first three letters of each time unit are parsed, you can use the singular or plural form of the word.</p>
<p>If you specify the following:</p>
<pre>-refreshtime 1 day 6 hours
</pre><p>Only those documents that were last indexed at least 30 hours and 1 second ago, are refreshed.</p>
<p><strong>Note: </strong>This option is valid only with the <code>-refresh</code> option. When you use <code>vsdb</code> <code>-recreate</code>, the last indexed date is cleared.</p><h3><a name="wp1000286"></a>-reparse</h3>
<p><strong>Type</strong>: Web crawling only</p>
<p>Forces parsing of all HTML documents already in the collection. You must specify a starting point with the <code>-start</code> option when you use the <code>-reparse</code> option.</p>
<p>You can use the <code>-reparse</code> option when you want to include paths and documents that were previously skipped due to exclusion or inclusion criteria. Remember to change the criteria, or there will be little for Verity Spider to do. This can be easy to overlook when you are using the <br /><code>-cmdfile</code> option.</p>
<h3><a name="wp1000280"></a>-unlimited</h3>
<p>Specifies that no limits are placed on Verity Spider if neither the <code>-host</code> nor the <code>-domain</code> option is specified. The default is to limit based on the host of the first starting point listed.</p>
<h3><a name="wp1000274"></a>-virtualhost</h3>
<p><strong>Syntax</strong>: <code>-virtualhost name_1 [name_n] ...</code></p>
<p>Specifies that DNS lookups are avoided for the hosts listed. You must use only complete text strings for hosts. You cannot use wildcard expressions. This lets you index by alias, such as when multiple web servers are running on the same host. You can use regular expressions.</p>
<p>Normally, when Verity Spider resolves host names, it uses DNS lookups to convert the names to canonical names, of which there can be only one per machine. This allows for the detection of duplicate documents, to prevent results from being diluted. In the case of multiple aliased hosts, however, duplication is not a barrier as documents can be referred to by more than one alias and yet remain distinct because of the different alias names.</p>
<h4><a name="wp1006526"></a>Example </h4>
<p>You can have both marketing.verity.com and sales.verity.com running on the same host. Each alias has a different document root, although document names such as index.htm can occur for both. With the <code>-virtualhost</code> option, both server aliases can be indexed as distinct sites. Without the <code>-virtualhost</code> option, they would both be resolved to the same host name, and only the first document encountered from any duplicate pair would be indexed.</p>
<p><strong>Note: </strong>If you are using Netscape Enterprise Server, and you have specified only the host name as a virtual host, Verity Spider will not be able to index the virtual host site. This is because Verity Spider always adds the domain name to the document key.</p>

<hr />
<p align="right"><p align="right"><a href="00001788.htm"><img src="images/previous.gif" width="9" height="14" border="0" alt="Previous"></a>&nbsp;&nbsp;<a href="00001791.htm"><img src="images/next.gif" width="9" height="14" border="0" alt="Next"></a>
</p></p>
<p><a href="http://livedocs.macromedia.com/coldfusion/7/htmldocs/00001790.htm" target="mm_window">View comments in LiveDocs</a></p>
 </body>
</html>



